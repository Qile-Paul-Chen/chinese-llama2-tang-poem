{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca98b0a9-08fe-40cd-92e5-7368bf16a421",
   "metadata": {},
   "source": [
    "## Finetune chinese-llama/alpaca for Tang poem writing\n",
    "\n",
    "In this notebook, we are going to use Chinese Tang poem data to finetune the Chinese llama/alpaca LLM for poem writing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb79063-5891-4e09-8ba1-9237a960225c",
   "metadata": {},
   "source": [
    "### 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c97803-fd94-472b-9ba7-877843df4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7 transformers[sentencepiece]==4.31.0 scipy tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2876d039-584f-4de6-88cf-560c48c1da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    HfArgumentParser,\n",
    "    LlamaTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa442783-2f6a-42a3-800f-ce4c7e0f8068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad414f-477b-41da-82fe-cd259e739557",
   "metadata": {},
   "source": [
    "### 2. Set up model and finetuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa038a0-b028-4688-b6bf-921712581b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"ziqingyang/chinese-alpaca-2-13b\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"chenqile09/tang-poems-with-keywords\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"chinese-alpaca-2-13B-tang-poem\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 1000\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 50\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd39608-c95e-46e2-8963-72cbc7f9dce9",
   "metadata": {},
   "source": [
    "### 3. Load dataset, tokenizer, and base model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9475e29-3a27-42f5-9903-3dfc4704c9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f05702ce144f839ca3f181566672b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b31cd15df045758807188f57b79003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11974eddab0c4191b13e34023e3844cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74da97eedeb140c3a23d34a7dbdeb6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24749657292a4585819f73752e8fbe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a84921da0be4f799d337ba05129e5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e29d993eb34b5496df8067bdbb5526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3b669e-a776-443b-b443-4dab04c301f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debe1d40ec364d3c87936c78ace3f400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/844k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eae228e99ee4b6089a8d61d1ce2af8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d06fb1ff054bd98e6f572932c0605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/766 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab286f6-befb-4467-9771-18e36887f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "print(compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e44a49c-a427-4cf9-a746-e0222297e71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8dc6d5-22c3-4e4b-882d-d96335c8f012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77be36ac40f34397bd13f640a852acc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4407e0af27431ab43c42e10c069f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ce45901c4941b19c4674ac014b2287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132f437b69394c09a9fbbbfa9537ad11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f5a935b89f41cb88d73ee69939aaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab67053be2e74801bab9af7ff0a6002f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f889e3061fed4235b8081ffe1e106363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07485c2dafd470d8e0a261fd53fadcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7fcb8ee-99f7-4458-a71c-260f8d583ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 28 02:29:58 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              36W / 250W |  10761MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check GPU memory usage (optional)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b7a1a-f3ae-4baa-866b-c023c3c190a3",
   "metadata": {},
   "source": [
    "### 4. Load finetuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5492413c-be50-4e21-8fb1-d2051849dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91c52e59-860d-4152-b7f6-db017582fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a518c79c-e016-419a-bcd8-fd10c6cc376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e2f05-af28-47fb-95e0-d498ff1dbb2e",
   "metadata": {},
   "source": [
    "### 5. Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2849d0c4-e6ce-404c-b8cd-162a7003c67e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:48:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.457700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.293400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>2.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>2.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>2.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>2.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>2.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>2.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>2.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>2.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>2.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>2.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>2.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>2.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>2.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>2.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>2.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>2.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>2.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>2.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>2.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>2.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>2.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>2.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>2.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>2.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>2.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>2.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>2.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>2.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>2.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>2.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.282100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=2.3109449827406143, metrics={'train_runtime': 6523.687, 'train_samples_per_second': 5.518, 'train_steps_per_second': 1.38, 'total_flos': 1.2958266472341504e+17, 'train_loss': 2.3109449827406143, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# if the model training stops in the middle, use the following to resume training from the checkpoint\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-3525\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9f28bee-b436-4a5f-8616-b785ded0f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b260f-b73b-483f-bade-04ce9a9666d1",
   "metadata": {},
   "source": [
    "### 6. Merge model in FP16 and merge it with LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c57a03b2-7d57-4af3-be8a-db199e64d0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae5a98f61194135a2f825ba88f09db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293d60e-3f27-475b-841b-0fc9ffe2c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need huggingface-cli login\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a8c05f2-d95b-430c-8d59-827beb410e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3b9e56d8bb4680a1bf16a1901d29f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe039caf7969424d95c02b59dde88748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8def5ecb3d2f487892f29a579456ffc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ec420e15de4b62865f63c0e7cec4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbd9e5ab80a47f7936822eba486c4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/844k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/chenqile09/chinese-alpaca-2-13B-tang-poem/commit/d20e8d47b6cdd82b0f8675da1ca115cf7772819a', commit_message='Upload tokenizer', commit_description='', oid='d20e8d47b6cdd82b0f8675da1ca115cf7772819a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push the model and tokenizer to hugging face\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62913465-71af-45cc-a99b-24fef5762a77",
   "metadata": {},
   "source": [
    "### 7. Set up eval configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a59157a-1268-4cbb-9bf5-72f72adda2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9004972-0c1b-4602-a651-8b4a9bbf3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenizer and model with QLoRA configuration\n",
    "# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# print(compute_dtype)\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=use_4bit,\n",
    "#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=use_nested_quant,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4db664-bce7-41c5-8331-5119e3a69c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load base model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'chenqile09/chinese-alpaca-2-LoRA-13B-couplet-100k',\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63235fa9-2477-453f-a795-9046ad9a3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load base model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     new_model,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4493421-f162-4cd5-8aad-d5be86580778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tokenizer\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('chenqile09/chinese-alpaca-2-LoRA-13B-couplet-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1658370-9a9a-4eef-a8e4-e994ed6e135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the text generation config\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "146911ad-5d13-41ad-81db-37428caedea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"你是一个写唐诗的AI助手\"\"\"\n",
    "\n",
    "TEMPLATE = (\n",
    "    \"<s>[INST] <<SYS>> {system_prompt} <</SYS>> 请以{keywords}为主题，作一首唐诗。[/INST]\"\n",
    ")\n",
    "\n",
    "def generate_prompt(keywords_list, system_prompt=SYSTEM_PROMPT):\n",
    "    return [TEMPLATE.format_map({'keywords': keywords,'system_prompt': system_prompt}) for keywords in keywords_list]\n",
    "\n",
    "def generate_output_from_llm(model, tokenizer, generation_config, keywords, system_prompt=SYSTEM_PROMPT):\n",
    "\n",
    "    input_texts = generate_prompt(keywords, system_prompt)\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True)\n",
    "    generation_outputs = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"].to(device),\n",
    "        attention_mask = inputs['attention_mask'].to(device),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        generation_config = generation_config\n",
    "    )\n",
    "\n",
    "    outputs = [tokenizer.decode(s, skip_special_tokens=True) for s in generation_outputs]\n",
    "    responses = [output.split(\"[/INST]\")[-1].strip() for output in outputs]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79c0f443-5436-4d5d-9f46-cff95a3cbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example inputs\n",
    "keywords_list = [\n",
    "    \"天涯，芳草，思念\",\n",
    "    \"闲日，无年，高树\",\n",
    "    \"上海，武汉，思念\",\n",
    "    \"春日，悠闲，快乐\",\n",
    "    \"帝王将相，围猎，壮志未酬\",\n",
    "    \"佛，心，亲\",\n",
    "    \"猫咪，亲昵，思念\",\n",
    "    \"粪坑，眼镜，裤子\",\n",
    "    \"灯光，幻象，女人\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60a671f2-74cd-4e0a-be0c-4eb2bb75b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天涯，芳草，思念\n",
      "南国天涯去不还，芳草萋萋绿满山。回首望乡心更苦，不知何处是家关。\n",
      "\n",
      "闲日，无年，高树\n",
      "闲日何曾有，无年亦不长。高树常相望，流水自潺湲。\n",
      "\n",
      "上海，武汉，思念\n",
      "汉水东流入大海，楚人南望见吴台。君王欲问何处去？妾家江上一帆开。\n",
      "\n",
      "春日，悠闲，快乐\n",
      "柳絮飘飞满院香，莺啼燕语入帘长。何当踏破青苔去，笑向东园看牡丹。\n",
      "\n",
      "帝王将相，围猎，壮志未酬\n",
      "汉家天子出龙门，百辟随从入苑前。将军若有雄心在，何须陪君到此边。\n",
      "\n",
      "佛，心，亲\n",
      "无相不离佛，有相是心身。若能见此理，即为佛与亲。\n",
      "\n",
      "猫咪，亲昵，思念\n",
      "猫儿爱睡时，爪尖枕上起。若见主人归，便来卧膝侧。\n",
      "\n",
      "粪坑，眼镜，裤子\n",
      "粪坑是何物？玻璃似眼镜。裤裆如铜钱，不须穿裤子。\n",
      "\n",
      "灯光，幻象，女人\n",
      "灯下人影似仙郎，画屏红烛照双光。不知是梦还是真，一曲笙歌欲晓长。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate responses in batch, and then print out\n",
    "\n",
    "responses = generate_output_from_llm(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    generation_config,\n",
    "    keywords_list,\n",
    "    SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "for keywords, response in zip(keywords_list, responses):\n",
    "    print(keywords+'\\n'+response+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0486c2-0012-47a5-93ab-681d50b5653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 天涯，芳草，思念\n",
    "# 南国天涯去不还，芳草萋萋绿满山。回首望乡心更苦，不知何处是家关。\n",
    "\n",
    "# 闲日，无年，高树\n",
    "# 闲日何曾有，无年亦不长。高树常相望，流水自潺湲。\n",
    "\n",
    "# 上海，武汉，思念\n",
    "# 汉水东流入大海，楚人南望见吴台。君王欲问何处去？妾家江上一帆开。\n",
    "\n",
    "# 春日，悠闲，快乐\n",
    "# 柳絮飘飞满院香，莺啼燕语入帘长。何当踏破青苔去，笑向东园看牡丹。\n",
    "\n",
    "# 帝王将相，围猎，壮志未酬\n",
    "# 汉家天子出龙门，百辟随从入苑前。将军若有雄心在，何须陪君到此边。\n",
    "\n",
    "# 佛，心，亲\n",
    "# 无相不离佛，有相是心身。若能见此理，即为佛与亲。\n",
    "\n",
    "# 猫咪，亲昵，思念\n",
    "# 猫儿爱睡时，爪尖枕上起。若见主人归，便来卧膝侧。\n",
    "\n",
    "# 粪坑，眼镜，裤子\n",
    "# 粪坑是何物？玻璃似眼镜。裤裆如铜钱，不须穿裤子。\n",
    "\n",
    "# 灯光，幻象，女人\n",
    "# 灯下人影似仙郎，画屏红烛照双光。不知是梦还是真，一曲笙歌欲晓长。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
